{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TIUR — AdamW Deep Dive: Time-to-Loss + LR Sweep\n",
        "\n",
        "This notebook is designed to answer the practical question:\n",
        "\n",
        "**Does a TIUR-motivated training trick (e.g., loss-mixed sampling) improve _time-to-loss_ under AdamW?**\n",
        "\n",
        "It runs a small **AdamW LR sweep** for:\n",
        "- `baseline` (IID)\n",
        "- `lossmixed` (loss-mixed sampling)\n",
        "\n",
        "Then it computes:\n",
        "- **steps-to-target** (for several loss thresholds)\n",
        "- **loss AUC** (area under the loss curve)\n",
        "- TIUR summaries (final churn, directed, efficiency)\n",
        "\n",
        "All artifacts are written to **Google Drive** so you don't lose them if the instance dies.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drive_mount"
      },
      "source": [
        "#@title 1) Mount Google Drive (outputs persist here)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, datetime\n",
        "OUT_ROOT = '/content/drive/MyDrive/tiur_tricks_results_adam'\n",
        "run_id = datetime.datetime.now().strftime('run_%Y%m%d_%H%M%S')\n",
        "out_dir = os.path.join(OUT_ROOT, run_id)\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "print('Outputs will be saved to:', out_dir)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deps"
      },
      "source": [
        "#@title 2) Install deps (avoid reinstalling torch/torchvision in Colab)\n",
        "# If you previously ran a cell that installed torchvision, restart runtime first.\n",
        "!pip install -q tqdm pandas matplotlib\n",
        "\n",
        "import torch\n",
        "print('torch:', torch.__version__)\n",
        "print('cuda available:', torch.cuda.is_available())\n",
        "!nvidia-smi -L\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unzip_repo"
      },
      "source": [
        "#@title 3) Unzip repo (set the zip path if needed)\n",
        "import os\n",
        "\n",
        "# Option A: keep the zip on Drive for convenience\n",
        "REPO_ZIP = '/content/drive/MyDrive/tiur_tricks_colab_v4.zip'  # <- change if your zip has a different name\n",
        "REPO_DIR = '/content/tiur_tricks_colab'\n",
        "\n",
        "if not os.path.isdir(REPO_DIR):\n",
        "    assert os.path.exists(REPO_ZIP), f'Could not find {REPO_ZIP}. Upload the zip to Drive or change REPO_ZIP.'\n",
        "    !unzip -q \"$REPO_ZIP\" -d /content/\n",
        "\n",
        "assert os.path.exists(os.path.join(REPO_DIR, 'tiur_tricks', '__init__.py')), 'Repo did not unzip correctly.'\n",
        "print('Repo ready at:', REPO_DIR)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "#@title 4) Robust imports (ensure we import *your* tiur_tricks)\n",
        "import sys\n",
        "if REPO_DIR not in sys.path:\n",
        "    sys.path.insert(0, REPO_DIR)\n",
        "\n",
        "# Clear stale import if you re-ran cells\n",
        "if 'tiur_tricks' in sys.modules:\n",
        "    del sys.modules['tiur_tricks']\n",
        "\n",
        "import tiur_tricks\n",
        "print('tiur_tricks loaded from:', getattr(tiur_tricks, '__file__', None))\n",
        "\n",
        "from tiur_tricks import RunConfig, run_experiment_suite\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "config"
      },
      "source": [
        "#@title 5) Experiment config (small enough for Colab, meaningful enough for curves)\n",
        "import math\n",
        "\n",
        "# Core knobs\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "DATASET = 'cifar10'\n",
        "MODEL = 'small_cnn'  # try 'resnet18' later\n",
        "\n",
        "# Make runs long enough that time-to-target is meaningful\n",
        "EPOCHS = 10\n",
        "SUBSET_TRAIN = 10000\n",
        "SUBSET_EVAL = 2000\n",
        "\n",
        "# TIUR stability: more replicates = better estimates, but slower\n",
        "NUM_REPLICATES = 5\n",
        "\n",
        "# Checkpoint frequency (controls plot resolution + overhead)\n",
        "CHECKPOINT_EVERY = 100\n",
        "EVAL_BATCHES = 20\n",
        "\n",
        "# AdamW LR sweep (keep it small for Colab)\n",
        "LR_LIST = [1e-3, 3e-3, 1e-2]\n",
        "WEIGHT_DECAY = 0.0\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# Optional extras\n",
        "INCLUDE_NOISE = False\n",
        "NOISE_STDS = [1e-3]\n",
        "\n",
        "print('DEVICE:', DEVICE)\n",
        "print('LR_LIST:', LR_LIST)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "build_suite"
      },
      "source": [
        "#@title 6) Build the AdamW suite: baseline vs lossmixed (plus optional noise)\n",
        "from dataclasses import replace\n",
        "\n",
        "base = RunConfig(\n",
        "    name='baseline',\n",
        "    device=DEVICE,\n",
        "    dataset=DATASET,\n",
        "    model=MODEL,\n",
        "    optimizer='adamw',\n",
        "    sampler='iid',\n",
        "    lr=LR_LIST[0],\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    num_replicates=NUM_REPLICATES,\n",
        "    subset_train=SUBSET_TRAIN,\n",
        "    subset_eval=SUBSET_EVAL,\n",
        "    checkpoint_every=CHECKPOINT_EVERY,\n",
        "    eval_batches=EVAL_BATCHES,\n",
        ")\n",
        "\n",
        "suite = []\n",
        "for lr in LR_LIST:\n",
        "    suite.append(replace(base, name=f'baseline_lr{lr:g}', sampler='iid', lr=lr))\n",
        "    suite.append(replace(base, name=f'lossmixed_lr{lr:g}', sampler='loss_mixed', lr=lr))\n",
        "\n",
        "if INCLUDE_NOISE:\n",
        "    for lr in LR_LIST:\n",
        "        for ns in NOISE_STDS:\n",
        "            suite.append(replace(base, name=f'noise{ns:g}_lr{lr:g}', sampler='iid', lr=lr, grad_noise_std=ns))\n",
        "\n",
        "print('Number of runs in suite:', len(suite))\n",
        "print('Runs:')\n",
        "for c in suite:\n",
        "    print(' -', c.name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_suite"
      },
      "source": [
        "#@title 7) Run the suite (saves everything to Drive)\n",
        "import time\n",
        "t0 = time.time()\n",
        "\n",
        "logs_df, summary_df = run_experiment_suite(\n",
        "    suite,\n",
        "    out_dir=out_dir,\n",
        "    show_plots=False,\n",
        "    save_plots=True,\n",
        "    persist_checkpoints=True,\n",
        ")\n",
        "\n",
        "print('Total wall time (min):', (time.time()-t0)/60)\n",
        "display(summary_df.sort_values('final_loss'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "analysis"
      },
      "source": [
        "#@title 8) Time-to-loss + AUC analysis + plots\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Ensure we have the full logs (mean curve per run)\n",
        "if logs_df is None or len(logs_df)==0:\n",
        "    logs_df = pd.read_csv(os.path.join(out_dir, 'all_logs.csv'))\n",
        "if summary_df is None or len(summary_df)==0:\n",
        "    summary_df = pd.read_csv(os.path.join(out_dir, 'summary.csv'))\n",
        "\n",
        "logs_df = logs_df.sort_values(['name','step'])\n",
        "summary_df = summary_df.copy()\n",
        "\n",
        "# Targets: baseline final loss at each LR, plus a couple absolute targets\n",
        "baseline_finals = summary_df[summary_df['name'].str.startswith('baseline_lr')].set_index('name')['final_loss'].to_dict()\n",
        "\n",
        "abs_targets = [1.90, 1.87, 1.85, 1.83]  # adjust as you like\n",
        "\n",
        "def steps_to_target(run_df, target):\n",
        "    hit = run_df[run_df['loss_mean'] <= target]\n",
        "    return float(hit['step'].iloc[0]) if len(hit) else np.inf\n",
        "\n",
        "def loss_auc(run_df):\n",
        "    x = run_df['step'].to_numpy()\n",
        "    y = run_df['loss_mean'].to_numpy()\n",
        "    if len(x) < 2:\n",
        "        return np.nan\n",
        "    return float(np.trapz(y, x) / (x[-1]-x[0]))  # normalized AUC\n",
        "\n",
        "rows = []\n",
        "for name, g in logs_df.groupby('name'):\n",
        "    g = g.sort_values('step')\n",
        "    row = {'name': name, 'loss_auc': loss_auc(g)}\n",
        "    for tgt in abs_targets:\n",
        "        row[f'steps_to_{tgt:.2f}'] = steps_to_target(g, tgt)\n",
        "    rows.append(row)\n",
        "extra = pd.DataFrame(rows)\n",
        "\n",
        "merged = summary_df.merge(extra, on='name', how='left')\n",
        "\n",
        "# Pick best LR per method family by final_loss and by AUC\n",
        "merged['family'] = merged['name'].apply(lambda s: 'lossmixed' if s.startswith('lossmixed_') else ('baseline' if s.startswith('baseline_') else 'other'))\n",
        "\n",
        "best_by_final = merged[merged['family'].isin(['baseline','lossmixed'])].sort_values('final_loss').groupby('family').head(1)\n",
        "best_by_auc = merged[merged['family'].isin(['baseline','lossmixed'])].sort_values('loss_auc').groupby('family').head(1)\n",
        "\n",
        "print('Best by final_loss:')\n",
        "display(best_by_final[['name','final_loss','final_efficiency','final_churn_frac','churn_integral','directed_integral','loss_auc']])\n",
        "\n",
        "print('Best by loss_auc (lower is better):')\n",
        "display(best_by_auc[['name','final_loss','final_efficiency','final_churn_frac','churn_integral','directed_integral','loss_auc']])\n",
        "\n",
        "# Save merged table\n",
        "merged_path = os.path.join(out_dir, 'adam_time_to_loss_summary.csv')\n",
        "merged.to_csv(merged_path, index=False)\n",
        "print('Saved:', merged_path)\n",
        "\n",
        "# Plot loss curves for the best-by-final-loss runs\n",
        "plt.figure(figsize=(7,4))\n",
        "for _, r in best_by_final.iterrows():\n",
        "    name = r['name']\n",
        "    g = logs_df[logs_df['name']==name].sort_values('step')\n",
        "    plt.plot(g['step'], g['loss_mean'], label=name)\n",
        "plt.xlabel('step')\n",
        "plt.ylabel('eval loss (mean over replicates)')\n",
        "plt.title('Best runs (by final loss) — loss vs step')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "fig1 = os.path.join(out_dir, 'best_final_loss_curves.png')\n",
        "plt.savefig(fig1, dpi=160, bbox_inches='tight')\n",
        "plt.show()\n",
        "print('Saved plot:', fig1)\n",
        "\n",
        "# Plot steps-to-target comparison at a chosen absolute target (if reached)\n",
        "target = 1.85\n",
        "col = f'steps_to_{target:.2f}'\n",
        "sub = merged[merged['family'].isin(['baseline','lossmixed'])].sort_values(['family','name'])[['name','family',col,'final_loss','loss_auc']]\n",
        "sub = sub.replace([np.inf], np.nan)\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "x = np.arange(len(sub))\n",
        "plt.bar(x, sub[col].fillna(0.0))\n",
        "plt.xticks(x, sub['name'], rotation=45, ha='right')\n",
        "plt.ylabel(f'steps to reach loss <= {target:.2f} (0 if never reached)')\n",
        "plt.title('Time-to-target (steps)')\n",
        "plt.grid(True, axis='y', alpha=0.3)\n",
        "fig2 = os.path.join(out_dir, f'steps_to_{target:.2f}.png')\n",
        "plt.savefig(fig2, dpi=160, bbox_inches='tight')\n",
        "plt.show()\n",
        "print('Saved plot:', fig2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next step if you still see “no time-to-loss benefit”\n",
        "\n",
        "If lossmixed ends lower but doesn’t reach intermediate thresholds faster, the most common reasons are:\n",
        "\n",
        "1. **Baseline LR is already optimal in this tiny regime**, so lossmixed can’t show a speedup without changing the stability boundary.\n",
        "2. **Lossmixed mostly helps late training** (better asymptote), not early convergence.\n",
        "\n",
        "Two practical follow-ups:\n",
        "- **Increase the LR grid upward** (e.g., `[1e-3, 3e-3, 1e-2, 2e-2]`) and see if lossmixed stays stable at higher LR.\n",
        "- **Try a larger model** (`resnet18`) and slightly more steps. Many “training tricks” don’t show clean speedups at very small scale.\n",
        "\n",
        "This notebook already supports that: just change `LR_LIST`, `MODEL`, and/or `EPOCHS`.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "name": "TIUR_Adam_TimeToLoss.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}